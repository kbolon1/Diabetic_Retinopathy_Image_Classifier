{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0b7a5f",
   "metadata": {},
   "source": [
    "This set of images includes all conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84cf4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras import models, layers\n",
    "#from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "#from keras.preprocessing.image import load_img\n",
    "#from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, decode_predictions #preprocess_input, \n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "#from tensorflow.keras.applications import mobilenet_v2\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#from matplotlib import pyplot\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing import image # Keras own inbuild image class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80abaa",
   "metadata": {},
   "source": [
    "# import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9885e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('data.xlsx', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9eacfb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df_full = pd.read_csv('full_df.csv', index_col=0)\n",
    "df.columns = df.columns.str.replace(' ','_').str.lower()\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804bb84",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecdeaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conditions(dataframe, condition_name):\n",
    "    \"\"\"to extract rows with specific attributes (normal, diabetic, etc)\n",
    "    \n",
    "    df to apply sort_df on\n",
    "    condition_name (str) list of conditions per df (n:normal, d: diabetic, \n",
    "      g: glaucoma, c: cataract, a: age-related, h: hypertensive, m: myoppia, \n",
    "      o: other )\n",
    "    \n",
    "    Returns the rows in df that meet the condition\"\"\"\n",
    "    \n",
    "    return dataframe[dataframe[condition_name] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de236f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eyes(dataframe, column, search_param):\n",
    "    \"\"\"to extract rows with specific attributes for eyes\n",
    "    \n",
    "    df to apply extract_eyes on\n",
    "    column: the column to apply condition to\n",
    "    search_param (str): the string to search on\n",
    "    \n",
    "    Returns the rows in df that meet the condition\"\"\"\n",
    "    return dataframe[dataframe[column].str.contains(search_param)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d58d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list(dataframe_1, dataframe_2):\n",
    "    \"\"\"make a list of images for to pull from image pool for specific attributes\n",
    "    \n",
    "    dataframe_1: left eye df\n",
    "    dataframe_2: right eye df\"\"\"\n",
    "\n",
    "    return list(dataframe_1['left-fundus'])+list(dataframe_2['right-fundus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f2ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_list(dataframe):\n",
    "    \"\"\"make a list of images for to pull from image pool for specific attributes\n",
    "    \n",
    "    dataframe_1: left eye df\n",
    "    dataframe_2: right eye df\"\"\"\n",
    "\n",
    "    return list(dataframe['left-fundus'])+list(dataframe['right-fundus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5241a4c",
   "metadata": {},
   "source": [
    "# Seperate Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae619d",
   "metadata": {},
   "source": [
    "## Normal Eye Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9374b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_full=extract_conditions(df, 'n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892047c",
   "metadata": {},
   "source": [
    "### Left Eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7026e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_left=extract_eyes(normal_full, 'left-diagnostic_keywords','normal fundus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "705e2a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_left.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5940ad57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1136"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_left['left-fundus'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7594453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked to see how many unique key diagnostic labels\n",
    "nl=normal_left.groupby(['left-diagnostic_keywords']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fbf6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove any other conditions, pure sample\n",
    "normal_left2=normal_left[~normal_left['left-diagnostic_keywords'].str.contains('dust')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4cbb37",
   "metadata": {},
   "source": [
    "### Right Eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbdbfcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_right=extract_eyes(normal_full, 'right-diagnostic_keywords','normal fundus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c89262f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked to see how many unique key diagnostic labels\n",
    "nr=normal_right.groupby(['right-diagnostic_keywords']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10a2c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove any other conditions, pure sample\n",
    "normal_right2=normal_right[~normal_right['right-diagnostic_keywords'].str.contains('dust')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06eb8c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2058"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_list_images=make_list(normal_left2, normal_right2)\n",
    "len(normal_list_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91babf",
   "metadata": {},
   "source": [
    "## Diabetic Eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5051f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_full=extract_conditions(df, 'd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74ccfda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1128, 14)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetic_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cca7c9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2256"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a list of images for diabetic sample\n",
    "diabetic_list_images=make_full_list(diabetic_full)\n",
    "len(diabetic_list_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a21f2",
   "metadata": {},
   "source": [
    "## Glaucoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da7f973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glaucoma_full=extract_conditions(df, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fc31b2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215, 14)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glaucoma_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85c5da20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a list of images for glaucoma sample\n",
    "glaucoma_list_images=make_full_list(glaucoma_full)\n",
    "len(glaucoma_list_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99acbcd8",
   "metadata": {},
   "source": [
    "## Cataracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "093cba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cataracts_full=extract_conditions(df, 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6aa5be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 14)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cataracts_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "722c285d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a list of images for cataracts sample\n",
    "cataracts_list_images=make_full_list(cataracts_full)\n",
    "len(cataracts_list_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb440286",
   "metadata": {},
   "source": [
    "## Age-Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3b60242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 14)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_full=extract_conditions(df, 'a')\n",
    "age_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be511204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a list of images for age-related sample\n",
    "age_list_images=make_full_list(age_full)\n",
    "len(age_list_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85455a2",
   "metadata": {},
   "source": [
    "## Hypertensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a1eb726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 14)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypertensive_full=extract_conditions(df, 'h')\n",
    "hypertensive_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f90e94bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a list of images for hypertensive sample\n",
    "hypertensive_list_images=make_full_list(hypertensive_full)\n",
    "len(hypertensive_list_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6c120",
   "metadata": {},
   "source": [
    "## Myopia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b33146dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174, 14)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myopia_full=extract_conditions(df, 'm')\n",
    "myopia_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8a86ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a list of images for myopic sample\n",
    "myopia_list_images=make_full_list(myopia_full)\n",
    "len(myopia_list_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bac30c",
   "metadata": {},
   "source": [
    "## Other Diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46c89af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(979, 14)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_full=extract_conditions(df, 'o')\n",
    "other_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61a94c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1958"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a list of images for other sample\n",
    "other_list_images=make_full_list(other_full)\n",
    "len(other_list_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1cbb72",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc129efc",
   "metadata": {},
   "source": [
    "# Split test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba3ea54",
   "metadata": {},
   "source": [
    "## Import Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60c93ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['normal', 'diabetic', 'age_related', 'glaucoma', 'other', 'hypertensive', 'myopia', 'cataract']\n",
    "base_path = 'oversampling/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2a435bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an image data generator\n",
    "data_gen = preprocessing.image.ImageDataGenerator(\n",
    "    # define the preprocessing function that should be applied to all images\n",
    "    preprocessing_function=keras.applications.vgg19.preprocess_input,\n",
    "    rescale=1/255,\n",
    "    # fill_mode='nearest',\n",
    "    # rotation_range=20,\n",
    "    # width_shift_range=0.2,\n",
    "    # height_shift_range=0.2,\n",
    "    # horizontal_flip=True, \n",
    "    # zoom_range=0.2,\n",
    "    # shear_range=0.2    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd32bdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# a generator that returns batches of X and y arrays\n",
    "train_data_gen = data_gen.flow_from_directory(\n",
    "        directory=base_path,\n",
    "        class_mode=\"categorical\",\n",
    "        classes=classes,\n",
    "        batch_size=4000,  ## note: it's really images: 100 per category. this is mostly only working for models with transfer learning\n",
    "        target_size=(224, 224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebd5e224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 224, 224, 3), (4000, 8))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in all images at once\n",
    "xtrain, ytrain = next(train_data_gen)\n",
    "xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e75d57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal',\n",
       " 'diabetic',\n",
       " 'age_related',\n",
       " 'glaucoma',\n",
       " 'other',\n",
       " 'hypertensive',\n",
       " 'myopia',\n",
       " 'cataract']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8df3de6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.dtype, ytrain.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f50f74",
   "metadata": {},
   "source": [
    "# VGG19 Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a832e0",
   "metadata": {},
   "source": [
    "tensorflow.keras.layers.Conv2D(filters, kernel_size, strides=(1, 1),\n",
    "  padding='valid', data_format=None, dilation_rate=(1, 1),\n",
    "  activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "  bias_initializer='zeros', kernel_regularizer=None,\n",
    "  bias_regularizer=None, activity_regularizer=None,\n",
    "  kernel_constraint=None, bias_constraint=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18115385",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg=keras.applications.vgg19.VGG19(\n",
    "    include_top=True,\n",
    "    weights=None, #'imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=None,\n",
    "    classes=8,\n",
    "    classifier_activation='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fdfc0a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 8)                 32776     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139,603,016\n",
      "Trainable params: 139,603,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check the summary:\n",
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e35cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_vgg.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "088810a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping:\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', #the thing we are monitoring\n",
    "    min_delta=0.005, #the minimum change in the quantity that we want for the model to train for another epoch\n",
    "    patience=20, #number of epochs with no improvement needed for the model to stop\n",
    "    verbose=1, #0 is silent, 1 means a message is displayed when something happens\n",
    "    mode='auto'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84ffd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "320/320 [==============================] - 360s 1s/step - loss: 2.0814 - accuracy: 0.1209 - val_loss: 2.0801 - val_accuracy: 0.1100\n",
      "Epoch 2/300\n",
      "320/320 [==============================] - 438s 1s/step - loss: 2.0797 - accuracy: 0.1247 - val_loss: 2.0806 - val_accuracy: 0.1062\n",
      "Epoch 3/300\n",
      "320/320 [==============================] - 660s 2s/step - loss: 2.0795 - accuracy: 0.1244 - val_loss: 2.0812 - val_accuracy: 0.1100\n",
      "Epoch 4/300\n",
      "320/320 [==============================] - 728s 2s/step - loss: 2.0795 - accuracy: 0.1247 - val_loss: 2.0815 - val_accuracy: 0.1062\n",
      "Epoch 5/300\n",
      "129/320 [===========>..................] - ETA: 7:00 - loss: 2.0790 - accuracy: 0.1364"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model_vgg.fit(xtrain,ytrain,batch_size=10, epochs=300,validation_split=0.2, callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f18904",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(300)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490774d",
   "metadata": {},
   "source": [
    "The training loss indicates how well the model is fitting the training data, while the validation loss indicates how well the model fits new data.\n",
    "\n",
    "Model is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466fdeff",
   "metadata": {},
   "source": [
    "# Image test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac217ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=Image.open('test_file/hypertensive/left_gen1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d66a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(width,height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadef38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning] *",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
